\section{Related Work}\label{sec:rw}

The random generation of test cases is a well established approach to the
compiler validation problem. Prior approaches are surveyed
in~\cite{Kossatchev2005,Boujarwah1997} and empirically contrasted
in~\cite{Chen2014a}. The main question of interest is in how to efficiently
generate codes which trigger bugs. There are two main approaches: \emph{program
generation}, where inputs are synthesized from scratch; and \emph{program
mutation}, where existing codes are modified so as to identify anomalous
behavior.

\begin{figure}
  \centering %
  \includegraphics[width=.85\columnwidth]{img/clang-crashes}%
  \vspace{-2em}
  \caption{%
  Crash rate of the Clang front-end of every LLVM release in the past 24
  months compiling 75k DeepSmith kernels.%
  }%
  \vspace{-1em}
  \label{fig:clangs}
  %
\end{figure}
\begin{table}
  \footnotesize %
  \centering %
  \caption{%
  The number of DeepSmith programs which trigger distinct Clang front-end
  assertions, and the number of programs which trigger unreachables.%
  }
  \vspace{-1.5em}
  \input{tab/clang-assert-counts}
  \label{tab:clangs}
\end{table}

\paragraph{Program Generation}

In the foundational work on differential testing for compilers, McKeeman
\emph{et al.\ }present generators capable of enumerating programs of a range of
qualities, from random ASCII sequences to C model conforming
programs~\cite{McKeeman1998}. Subsequent works have presented increasingly
complex generators which improve in some metric of interest, generally
expressiveness or probability of correctness. CSmith~\cite{Yang2011} is a widely
known and effective generator which enumerates programs by pairing infrequently
combined language features. In doing so, it produces correct programs with
clearly defined behavior but very unlikely functionality, increasing the chances
of triggering a bug. Achieving this required extensive engineering work, most of
it not portable across languages, and ignoring some language features.
Subsequent generators influenced by CSmith, like Orange3~\cite{Nagai2013}, focus
on features and bug types beyond the scope of CSmith, arithmetic bugs in the
case of Orange3. Glade~\cite{Bastani2017} derives a grammar from a corpus of
example programs. The derived grammar is enumerated to produce new programs,
though unlike our approach, no distribution is learned over the grammar; program
enumeration is uniformly random.

\paragraph{Program Mutation}

Equivalence Modulo Inputs (EMI) testing~\cite{Le2013a,Sun2016a} follows a
different approach to test case generation. Starting with existing code, it
inserts or deletes statements that will not be executed, so functionality should
remain the same. If it is affected, it is due to a compiler bug. While a
powerful technique able to find hard to detect bugs, it relies on having a very
large number of programs to mutate. As such, it still requires an external code
generator. Similarly to CSmith, EMI favors very long test programs.
LangFuzz~\cite{Holler2012} also uses mutation but does this by inserting code
segments which have previously exposed bugs. This increases the chances of
discovering vulnerabilities in scripting language engines. Skeletal program
enumeration~\cite{Zhang2017a} again works by transforming existing code. It
identifies algorithmic patterns in short pieces of code and enumerates all the
possible permutations of variable usage. Compared to all these, our fuzzing
approach is low cost, easy to develop, portable, capable of detecting a wide
range of errors, and focusing by design on bugs that are more likely to be
encountered in a production scenario.

\paragraph{Machine Learning}

There is an increasing interest in applying machine learning to software
testing. Most similar to our work is Learn\&fuzz~\cite{Godefroid2017}, in which
an LSTM network is trained over a corpus of PDF files to generate test inputs
for the Microsoft Edge renderer, yielding one bug. Unlike compiler testing, PDF
test cases require no inputs and no pre-processing of the training corpus.
Skyfire~\cite{Wang2017c} learns a probabilistic context-sensitive grammar over a
corpus of programs to generate input seeds for mutation testing. The generated
seeds are shown to improve the code coverage of AFL~\cite{Zalewski} when fuzzing
XSLT and XML engines, though the seeds are not directly used as test cases.
Machine learning has also been applied to other areas such as improving bug
finding static analyzers~\cite{Heo2017,Koc2017}, repairing
programs~\cite{Koukoutos2017a,White}, prioritizing test
programs~\cite{Chen2017}, identifying buffer overruns~\cite{Choi2016}, and
processing bug reports~\cite{Lam2016,Huo2016}. To the best of our knowledge, no
work so far has succeeded in finding compiler bugs by exploiting the learned
syntax of mined source code for test case generation. Ours is the first to do
so.


\begin{table}
  \footnotesize %
  \centering %
  \caption{%
  The number of DeepSmith programs that trigger Solidity compiler crashes from
  12 hours of testing.%
  \vspace{-1em}
  }
  \begin{tabular}{rc|ccc}
    \toprule
    \textbf{Compiler} & $\pm$ & \textbf{Silent Crashes} & \textbf{Assertion 1} & \textbf{Assertion 2}\\
    \midrule
    \multirow{ 2}{*}{solc}    & $-$ & 204 & 1 & \\
    & $+$ & 204 & 1 & \\
    \hline
    \multirow{ 2}{*}{solc-js} & $-$ & 3628 & 1 & 1\\
    & $+$ & 908 & 1 & 1\\
    \bottomrule
  \end{tabular}
  \vspace{-2em}
  \label{tab:solidity}
\end{table}
